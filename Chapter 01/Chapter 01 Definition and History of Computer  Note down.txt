=================== Computer was created at 1947 by two scientist name is Mauchly and Echert =========================
==> The first generation is from 1942-1955. They were based on vacuum tubes. The first computer machine named UNIVAC-1
was launched in 1951. This computer machine used magnetic medium for input/output of data. The technology used for 
those computers was valve Technology. The main limitation of this technology was heavy power consumption and not very
high reliability. Examples of first generation computers are ENIVAC and UNIVAC
==> Advantages and Disadvantages of Computer such as : 
+ Advantages of first generation computers
- Vacuum tubes were the only electronic component available during those days.
- Vacuum tube Technology made possible to make electronic digital computers.
- These computers could calculate data in milliseconds.
+ Disadvantages of first generation computers
- The computers were very large in size
- They consume a large amount of energy
- They heated very soon due to thousands of vacuum tubes
- They were not very reliable
- Air conditioning was required
- Constant maintenance was required
- Costly commercial productions
- Limited commercial use
- Very slow speed




























































































The Fifth Generation Computer Systems (FGCS) was an initiative by Japan's Ministry of International Trade and Industry (MITI), 
begun in 1982, to create computers using massively parallel computing and logic programming. It was to be the result of a 
government/industry research project in Japan during the 1980s. It aimed to create an "epoch-making computer" with supercomputer-
like performance and to provide a platform for future developments in artificial intelligence. There was also an unrelated Russian
project also named as a fifth-generation computer (see Kronos (computer)). Ehud Shapiro, in his "Trip Report" paper[1] (which 
focused the FGCS project on concurrent logic programming as the software foundation for the project), captured the rationale and motivations driving this project: "As part of Japan's effort to become a leader in the computer industry, the Institute for New Generation 
Computer Technology has launched a revolutionary ten-year plan for the development of large computer systems which will be applicable to knowledge information 
knowledge from abroad without contributing any of its own, this project will stimulate original research and will make its results available to the international 
research community." The term "fifth generation" was intended to convey the system as being advanced. In the history of computing hardware, computers using vacuum 
tubes were called the first generation; transistors and diodes, the second; integrated circuits, the third; and those using microprocessors, the fourth. Whereas 
previous computer generations had focused on increasing the number of logic elements in a single CPU, the fifth generation, it was widely believed at the time, 
would instead turn to massive numbers of CPUs for added performance.
The project would have created the computer over a ten-year period. After the project ended, it would consider an investment in a new "sixth generation" project. Opinions 
about its outcome are divided: either it was a failure, or it was ahead of its time.

==> Contents
1	Information
1.1	Implementation
1.2	Failure
1.3	Ahead of its time
2	References

==> Information: 

In the late 1965s it was one of the most used until the early 1970s, there was much talk about "generations" of computer hardware � usually "three generations".
First generation: Thermionic vacuum tubes. Mid-1940s. IBM pioneered the arrangement of vacuum tubes in pluggable modules. The IBM 650 was a first-generation computer.
Second generation: Transistors. 1956. The era of miniaturization begins. Transistors are much smaller than vacuum tubes, draw less power, and generate less heat. 
Discrete transistors are soldered to circuit boards, with interconnections accomplished by stencil-screened conductive patterns on the reverse side. The IBM 7090
was a second-generation computer.
Third generation: Integrated circuits (silicon chips containing multiple transistors). 1964. A pioneering example is the ACPX module used in the IBM 360/91, which, 
by stacking layers of silicon over a ceramic substrate, accommodated over 20 transistors per chip; the chips could be packed together onto a circuit board to 
achieve unprecedented logic densities. The IBM 360/91 was a hybrid second- and third-generation computer. Omitted from this taxonomy is the "zeroth-generation" 
computer based on metal gears (such as the IBM 407) or mechanical relays (such as the Mark I), and the post-third-generation computers based on Very Large Scale 
Integrated (VLSI) circuits. There was also a parallel set of generations for software:
First generation: Machine language.
Second generation: Low-level programming languages such as Assembly language.
Third generation: Structured high-level programming languages such as C, COBOL and FORTRAN.
Fourth generation: "Non-procedural" high-level programming languages (such as object-oriented languages)[2]

Throughout these multiple generations up to the 1970s, Japan built computers following U.S. and British leads. In the mid-1970s, the Ministry of International 
Trade and Industry stopped following western leads and started looking into the future of computing on a small scale. They asked the Japan Information Processing 
Development Center (JIPDEC) to indicate a number of future directions, and in 1979 offered a three-year contract to carry out more in-depth studies along with 
industry and academia. It was during this period that the term "fifth-generation computer" started to be used.
Prior to the 1970s, MITI guidance had successes such as an improved steel industry, the creation of the oil supertanker, the automotive industry, 
consumer electronics, and computer memory. MITI decided that the future was going to be information technology. However, the Japanese language, particularly in 
its written form, presented and still presents obstacles for computers.
[3] As a result of these hurdles, MITI held a conference to seek assistance from experts.
The primary fields for investigation from this initial project were:
Inference computer technologies for knowledge processing
Computer technologies to process large-scale data bases and knowledge bases
High performance workstations
Distributed functional computer technologies
Super-computers for scientific calculation
The project imagined an "epoch-making computer" with supercomputer-like performance using massively parallel computing/processing. The aim was to build parallel 
computers for artificial intelligence applications using concurrent logic programming. The FGCS project and its vast findings contributed greatly to the 
development of the concurrent logic programming field.
The target defined by the FGCS project was to develop "Knowledge Information Processing systems" (roughly meaning, applied Artificial Intelligence). The chosen 
tool to implement this goal was logic programming. Logic programming approach as was characterized by Maarten Van Emden � one of its founders � as:[4]
The use of logic to express information in a computer.
The use of logic to present problems to a computer.
The use of logical inference to solve these problems.
More technically, it can be summed up in two equations:
Program = Set of axioms.
Computation = Proof of a statement from axioms.
The Axioms typically used are universal axioms of a restricted form, called Horn-clauses or definite-clauses. The statement proved in a computation is an 
existential statement[citation needed]. The proof is constructive, and provides values for the existentially quantified variables: these values constitute the 
output of the computation. Logic programming was thought as something that unified various gradients of computer science 
(software engineering, databases, computer architecture and artificial intelligence). It seemed that logic programming was a key missing connection between 
knowledge engineering and parallel computer architectures. The project imagined a parallel processing computer running on top of large databases 
(as opposed to a traditional filesystem) using a logic programming language to define and access the data. They envisioned building a prototype machine with 
performance between 100M and 1G LIPS, where a LIPS is a Logical Inference Per Second. At the time typical work station machines were capable of about 100k LIPS. 
They proposed to build this machine over a ten-year period, 3 years for initial R&D, 4 years for building various subsystems, and a final 3 years to complete a 
working prototype system. In 1982 the government decided to go ahead with the project, and established the Institute for New Generation Computer Technology (ICOT)
through joint investment with various Japanese computer companies.
In the same year, during a visit to the ICOT, Ehud Shapiro invented Concurrent Prolog, a novel concurrent programming language that integrated logic programming 
and concurrent programming. Concurrent Prolog is a logic programming language designed for concurrent programming and parallel execution. It is a process oriented 
language, which embodies dataflow synchronization and guarded-command indeterminacy as its basic control mechanisms. Shapiro described the language in a Report 
marked as ICOT Technical Report 003,[5] which presented a Concurrent Prolog interpreter written in Prolog. Shapiro's work on Concurrent Prolog inspired a change 
in the direction of the FGCS from focusing on parallel implementation of Prolog to the focus on concurrent logic programming as the software foundation for the 
Sproject. It also inspired the concurrent logic programming language Guarded Horn Clauses (GHC) by Ueda, which was the basis of KL1, the programming language that
was finally designed and implemented by the FGCS project as its core programming language.
****************************************************************************************************************************************************************
==> Implementation

The belief that parallel computing was the future of all performance gains generated by the Fifth-Generation project produced a wave of apprehension in the 
computer field. After having influenced the consumer electronics field during the 1970s and the automotive world during the 1980s, the Japanese in the 1980s 
developed a strong reputation. Soon parallel projects were set up in the US as the Strategic Computing Initiative and the Microelectronics and Computer Technology
Corporation (MCC), in the UK as Alvey, and in Europe as the European Strategic Program on Research in Information Technology (ESPRIT), as well as the European 
Computer-Industry Research Centre (ECRC) in Munich, a collaboration between ICL in Britain, Bull in France, and Siemens in Germany.
Five running Parallel Inference Machines (PIM) were eventually produced: PIM/m, PIM/p, PIM/i, PIM/k, PIM/c. The project also produced applications to run on 
these systems, such as the parallel database management system Kappa, the legal reasoning system HELIC-II, and the automated theorem prover MGTP, as well as 
applications to bioinformatics.
****************************************************************************************************************************************************************
==> Failure

The FGCS Project did not meet with commercial success for reasons similar to the Lisp machine companies and Thinking Machines. The highly parallel computer 
architecture was eventually surpassed in speed by less specialized hardware (for example, Sun workstations and Intel x86 machines). The project did produce a new 
generation of promising Japanese researchers. But after the FGCS Project, MITI stopped funding large-scale computer research projects, and the research momentum 
developed by the FGCS Project dissipated. However MITI/ICOT embarked on a Sixth Generation Project in the 1990s.
A primary problem was the choice of concurrent logic programming as the bridge between the parallel computer architecture and the use of logic as a knowledge 
representation and problem solving language for AI applications. This never happened cleanly; a number of languages were developed, all with their own limitations.
In particular, the committed choice feature of concurrent constraint logic programming interfered with the logical semantics of the languages.[6]
Another problem was that existing CPU performance quickly pushed through the barriers that experts perceived in the 1980s, and the value of parallel computing 
dropped to the point where it was for some time used only in niche situations. Although a number of workstations of increasing capacity were designed and built 
over the project's lifespan, they generally found themselves soon outperformed by "off the shelf" units available commercially.
The project also failed to maintain continuous growth. During its lifespan, GUIs became mainstream in computers; the internet enabled locally stored databases 
to become distributed; and even simple research projects provided better real-world results in data mining.[citation needed] Moreover, the project found that 
the promises of logic programming were largely negated by the use of committed choice.[citation needed] 
At the end of the ten-year period, the project had spent over �50 billion (about US$400 million at 1992 exchange rates) and was terminated without having met its
goals. The workstations had no appeal in a market where general purpose systems could now replace and outperform them. This is parallel to the Lisp machine 
market, where rule-based systems such as CLIPS could run on general-purpose computers, making expensive Lisp machines unnecessary.[7]
Ahead of it.
Albeit not having produced much success, many of the approaches seen in the Fifth-Generation project, such as logic programming have been distributed over 
massive knowledge-bases, and are now being re-interpreted in current technologies. For example, the Web Ontology Language (OWL) employs several layers of 
logic-based knowledge representation systems. It appears, however, that these new technologies reinvented rather than leveraged approaches investigated under 
the Fifth-Generation initiative. In the early 21st century, many flavors of parallel computing began to proliferate, including multi-core architectures at the 
low-end and massively parallel processing at the high end. When clock speeds of CPUs began to move into the 3�5 GHz range, CPU power dissipation and other 
problems became more important. The ability of industry to produce ever-faster single CPU systems 
(linked to Moore's Law about the periodic doubling of transistor counts) began to be threatened. Ordinary consumer machines and game consoles began to have 
parallel processors like the Intel Core, AMD K10, and Cell. Graphics card companies like Nvidia and AMD began introducing large parallel systems like CUDA and 
OpenCL. Again, however, it is not clear that these developments were facilitated in any significant way by the Fifth-Generation project.
In summary, it is argued that the Fifth-Generation project was revolutionary, however, still had areas of downfall.[8]
****************************************************************************************************************************************************************
Characteristics of Fifth Generation of Computers
- Multi-processor based system: Currently we use one processor per CPU though there are special computers already in use with parallel computing but those are very
  limited and not complete.
- Use of Artificial Intelligence: AI is also in use already, but still it is in development. In fifth generation computers, we expect to see AI applied in everywhere, 
from navigating to browsing, from everyday word-excel sheet processing to heavy duty image processing and video analyzing etc. AI will become personal assistant, AI will 
automate almost every aspect computing. Use of optical fiber in circuits Development of the elements of programs Automated audio in any language to control the workflow of 
the computer Magnetic enabled chips  Huge development of storage: Already we have SSD storage which is way faster than HDD, and a few other technologies under development, 
thus we expect to faster and larger storage in fifth generation computers. More powerful micro and macro computers Development of enormous powers with AI
Read More: 10 characteristics of first generation of compute
****************************************************************************************************************************************************************
Fifth generation computer technology, based on artificial intelligence, is still in development, though there are some applications, such as voice recognition, 
that are being used today. The use of parallel processing and superconductors is helping to make artificial intelligence a reality. This is also so far the 
prime generation for packing a large amount of storage into a compact and portable device.
Quantum computation and molecular and nanotechnology will radically change the face of computers in years to come. The goal of fifth-generation computing is to 
develop devices that will respond to natural language input and are capable of learning and self-organization.
****************************************************************************************************************************************************************

